---
title: "Useful Reparameterizations"
author: "Brayden Tang"
date: "24/08/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Purpose

With HMC based MCMC sampling, difficulties often arise when attempting to sample from distributions that have complex geometries. The highly simplified, non-rigorous TLDR for why this happens is that the "ideal" step size (how long the random momentum is imparted on the sampler per each time step) in some regions of these distributions is less than ideal in other regions of the distribution. 

This mismatch in optimal step size leads to proposals that are within the same region of the distribution that the sampler currently is at, and/or inadequate exploration of specific regions of the distribution, leading to high autocorrelation within chains/inefficient sampling and/or biased posteriors.

There are two main culprits that create these complex geometries:

1) Highly correlated posterior parameters
    - the classic example being centered Normal parameterizations, commonly used when fitting hierarchical models
    - another example: intercept + predictor coefficients where predictors were not centered and scaled

2) Sampling from heavy tailed distributions
    - example: Cauchy, Student-T, Lognormal, Pareto, Inverse-Gaussian, etc.
    
To get around these issues, reparameterizing the model to explicitly sample from "geometrically friendly" distributions is key as other solutions (such as increasing adapt_delta or the maximum treedepth) come at the cost of increased computation time. A clever reparameterization doesn't have these issues and often leads to significant speedup and efficiency gains. The key idea is that we can utilize facts from probability theory by applying a specific transformation to a sample that comes from a "convenient" distributions. The resulting transformed sample can then be proven to come from the more complex distribution.

"Convenient" distributions are typically Uniform(0, 1), Gamma(a, b), or Normal(0, 1) distributions. HMC samplers typically don't have many issues with these distributions as their tails are rather regular.

This document is just a convenient lookup table for me so that I don't have to constantly scan Google for transformations that I only vaguely remember.

## Cauchy

## Student-T

## Normal(inferred mu, inferred sigma)

## Lognormal

## Pareto (Lomax)

## Inverse Gaussian

## Multivariate Normal 
